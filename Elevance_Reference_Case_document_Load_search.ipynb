{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXbGqCa1yD2hEinVsPu6Uh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sbomma1973/AppSearch_DataIngest-csv-Python/blob/master/Elevance_Reference_Case_document_Load_search.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# This program is to illurate how to Chunk and index a word/pdf document into elasticsearch. This includes to store the information not just as text but also as embeddings."
      ],
      "metadata": {
        "id": "EsPa6Mxv4zHr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step1: Download python libraries to convert docx into json. install and import libaries needed for this demo\n"
      ],
      "metadata": {
        "id": "B5Ej-I3e5O7m"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "howxYj4V4dMP",
        "outputId": "8edae54a-2f8e-4fd7-8363-c4e3e484f9c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-docx\n",
            "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting openai==0.28\n",
            "  Downloading openai-0.28.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.0)\n",
            "Requirement already satisfied: lxml>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.9.4)\n",
            "Requirement already satisfied: typing-extensions>=4.9.0 in /usr/local/lib/python3.10/dist-packages (from python-docx) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Downloading openai-0.28.0-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.5/76.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.3/244.3 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-docx, openai\n",
            "Successfully installed openai-0.28.0 python-docx-1.1.2\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.14.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting elastic-transport<9,>=8.13 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.13.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.7.4)\n",
            "Downloading elasticsearch-8.14.0-py3-none-any.whl (480 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.2/480.2 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elastic_transport-8.13.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.5/64.5 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: elastic-transport, elasticsearch\n",
            "Successfully installed elastic-transport-8.13.1 elasticsearch-8.14.0\n"
          ]
        }
      ],
      "source": [
        "!pip install python-docx openai==0.28\n",
        "!pip install elasticsearch\n",
        "\n",
        "\n",
        "from docx import Document\n",
        "from elasticsearch import Elasticsearch\n",
        "from elasticsearch import Elasticsearch\n",
        "import pandas as pd\n",
        "import openai\n",
        "from getpass import getpass\n",
        "import openai\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Setup Enviromment variables and Connection Parameters for Elasticsearch and OpenAI - Make sure your elastic search is up and running and you have an OpenAI account. If you are using some other LLM make sure you have connectivity for that\n"
      ],
      "metadata": {
        "id": "bbKfwm3e6Q06"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLOUD_ID=getpass(\"Enter Cloud ID: \")\n",
        "CLOUD_USER=getpass(\"Enter Cloud User:  \")\n",
        "CLOUD_PASS=getpass(\"Enter Cloud Password:  \")\n",
        "elastic_api_key = getpass(\"Enter Elastic API Key: \")\n",
        "\n",
        "\n",
        "openai.api_key =getpass(\"OpenAI API Key: \")\n",
        "\n",
        "es = Elasticsearch(cloud_id=CLOUD_ID, basic_auth=(CLOUD_USER,CLOUD_PASS))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PE3Wj2fe6Ksp",
        "outputId": "209f149c-44e9-41ac-f4e6-c6bdbff4de28"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter Cloud ID: ··········\n",
            "Enter Cloud User:  ··········\n",
            "Enter Cloud Password:  ··········\n",
            "Enter Elastic API Key: ··········\n",
            "OpenAI API Key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es = Elasticsearch(cloud_id=CLOUD_ID, basic_auth=(CLOUD_USER,CLOUD_PASS))\n",
        "document_path = '/content/sample_data/test.docx'\n",
        "index_name = 'cardiovascular_effective_care'\n"
      ],
      "metadata": {
        "id": "n41kcVwl8rCy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "es.indices.delete(index=index_name, ignore=[400,404])\n",
        "es.ingest.delete_pipeline(id=\"docx_pipeline\")\n",
        "\n",
        "pipeline_body = {\n",
        "    \"description\": \"Ingest pipeline to convert text to embeddings using Elser model\",\n",
        "    \"processors\": [\n",
        "        {\n",
        "            \"inference\": {\n",
        "                \"model_id\": \".elser_model_2\",\n",
        "                \"input_output\": [\n",
        "                    {\n",
        "                        \"input_field\": \"content\",\n",
        "                        \"output_field\": \"content_embedding\"\n",
        "                    }\n",
        "                ]\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create the ingest pipeline\n",
        "response = es.ingest.put_pipeline(id='docx_pipeline', body=pipeline_body)\n",
        "print(\"Pipeline creation response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDRLbV6efZyA",
        "outputId": "907a8298-8c46-437c-e968-416113d40f7e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-21b3cb9120c0>:1: DeprecationWarning: Passing transport options in the API method is deprecated. Use 'Elasticsearch.options()' instead.\n",
            "  es.indices.delete(index=index_name, ignore=[400,404])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pipeline creation response: {'acknowledged': True}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "index_settings = {\n",
        "    \"settings\": {\n",
        "        \"index\": {\n",
        "            \"default_pipeline\": \"docx_pipeline\"\n",
        "        }\n",
        "    },\n",
        "    \"mappings\": {\n",
        "        \"properties\": {\n",
        "            \"chunk_id\": {\n",
        "                \"type\": \"long\"\n",
        "            },\n",
        "            \"content\": {\n",
        "                \"type\": \"text\",\n",
        "                \"fields\": {\n",
        "                    \"keyword\": {\n",
        "                        \"type\": \"keyword\",\n",
        "                        \"ignore_above\": 256\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"content_embedding\": {\n",
        "                \"type\": \"sparse_vector\"\n",
        "            },\n",
        "            \"table\": {\n",
        "                \"properties\": {\n",
        "                    \"row\": {\n",
        "                        \"type\": \"text\",\n",
        "                        \"fields\": {\n",
        "                            \"keyword\": {\n",
        "                                \"type\": \"keyword\",\n",
        "                                \"ignore_above\": 256\n",
        "                            }\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            },\n",
        "            \"table_embedding\": {\n",
        "                \"type\": \"sparse_vector\"\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "if not es.indices.exists(index=index_name):\n",
        "    response = es.indices.create(index=index_name, body=index_settings)\n",
        "    print(\"Index creation response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UploxRBbfkvW",
        "outputId": "f8aa3bde-e4de-4840-f3c5-8a8236b5babe"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index creation response: {'acknowledged': True, 'shards_acknowledged': True, 'index': 'cardiovascular_effective_care'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reference Word Doc Strategy: The word doc, there is text and tables, i have indexed everything into content field in elastic and created embedddings using Elser(elastic Machine learning tool to create sparse embeddings)\n",
        "\n",
        "You can use any other tool to create embeddings such as (Open AI embeddings)\n",
        "\n"
      ],
      "metadata": {
        "id": "Gyel_q5KAJCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_text_and_tables_from_docx(file_path):\n",
        "    doc = Document(file_path)\n",
        "    full_text = []\n",
        "    tables = []\n",
        "    for para in doc.paragraphs:\n",
        "        full_text.append(para.text)\n",
        "\n",
        "    for table in doc.tables:\n",
        "        table_data = []\n",
        "        for row in table.rows:\n",
        "            row_data = [cell.text for cell in row.cells]\n",
        "            table_data.append(row_data)\n",
        "        tables.append(table_data)\n",
        "\n",
        "    return '\\n'.join(full_text), tables\n",
        "\n",
        "\n",
        "def chunk_text(text, chunk_size=256):\n",
        "    words = text.split()\n",
        "    chunks = [' '.join(words[i:i + chunk_size]) for i in range(0, len(words), chunk_size)]\n",
        "    return chunks\n",
        "\n",
        "def chunk_tables(tables, rows_per_chunk=10):\n",
        "    table_chunks = []\n",
        "    for table in tables:\n",
        "        for i in range(0, len(table), rows_per_chunk):\n",
        "            table_chunks.append(table[i:i + rows_per_chunk])\n",
        "    return table_chunks\n",
        "\n",
        "def index_chunks(chunks, index_name, es_client):\n",
        "\n",
        "  documents = []\n",
        "\n",
        "  for i, chunk in enumerate(text_chunks):\n",
        "        doc = {\n",
        "            'content': chunk,\n",
        "            'chunk_id': i\n",
        "        }\n",
        "        documents.append(doc)\n",
        "        #print (doc)\n",
        "\n",
        "  for i, table_chunk in enumerate(table_chunks):\n",
        "    # Ensure each item in table_chunk is a string\n",
        "    table_chunk_str = ' '.join(str(item) for item in table_chunk)\n",
        "    doc = {\n",
        "        'content': table_chunk_str,  # Concatenate rows into a single string\n",
        "        'chunk_id': len(text_chunks) + i\n",
        "    }\n",
        "    documents.append(doc)\n",
        "\n",
        "  for doc in documents:\n",
        "    es.index(index=index_name, body=doc)\n"
      ],
      "metadata": {
        "id": "EyfFbGZh5e6O"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chucking Strategy - I chunked the document you provided into 256 bytes per chunk - You can have different chunking strategys based on how granular or overlapping your search to be\n"
      ],
      "metadata": {
        "id": "iMerHqog_ft1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text, tables = extract_text_and_tables_from_docx(document_path)\n",
        "text_chunks = chunk_text(text, chunk_size=256)\n",
        "table_chunks = chunk_tables(tables, rows_per_chunk=100)\n",
        "\n",
        "#print(text)\n",
        "#print(tables)\n",
        "chunks = chunk_text(text)\n",
        "index_chunks(chunks, index_name, es)"
      ],
      "metadata": {
        "id": "Elledcxx9D1m"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken openai==0.28\n",
        "\n",
        "import openai\n",
        "import tiktoken\n",
        "\n",
        "def truncate_text(text, max_tokens):\n",
        "    tokens = text.split()\n",
        "    if len(tokens) <= max_tokens:\n",
        "        return text, len(tokens)\n",
        "\n",
        "    return ' '.join(tokens[:max_tokens]), len(tokens)\n",
        "\n",
        "def encoding_token_count(string: str, encoding_model: str) -> int:\n",
        "    encoding = tiktoken.encoding_for_model(encoding_model)\n",
        "    return len(encoding.encode(string))\n",
        "\n",
        "def chat_gpt(prompt, model=\"gpt-3.5-turbo\", max_tokens=1024, max_context_tokens=4000, safety_margin=5):\n",
        "    # Truncate the prompt content to fit within the model's context length\n",
        "    truncated_prompt, word_count = truncate_text(prompt, max_context_tokens - max_tokens - safety_margin)\n",
        "    openai_token_count = encoding_token_count(prompt, model)\n",
        "    #print(f\"word_count = {word_count}, openai_token_count = {openai_token_count}\")\n",
        "\n",
        "    # Updated way to call the ChatCompletion API in openai>=1.0.0\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are an Elevance's Claims Expert and understand Medical Terminology\"},\n",
        "            {\"role\": \"user\", \"content\": truncated_prompt}\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Access the message content from the response\n",
        "    return response.choices[0].message.content, word_count, openai_token_count\n",
        "    return response[\"choices\"][0][\"message\"][\"content\"], word_count, openai_token_count\n",
        "\n",
        "def search1(query2):\n",
        "    response = es.search(\n",
        "        index=\"cardiovascular_effective_care\",\n",
        "        size=1,\n",
        "        query={\n",
        "            \"text_expansion\": {\n",
        "                \"content_embedding\": {\n",
        "                    \"model_id\": \".elser_model_2\",\n",
        "                    \"model_text\": query2\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "    )\n",
        "    content = []\n",
        "    for hit in response[\"hits\"][\"hits\"]:\n",
        "        content = hit[\"_source\"][\"content\"]\n",
        "\n",
        "        print('Results from elastic:', content)\n",
        "    return content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OFO7HTgCqLdE",
        "outputId": "a28680f0-dc6c-44fc-92ff-524e6a988f4d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.4)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (2024.7.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.3.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OPenAI connector - with OpenAI API key with gpt-3.5-turbo (Cheapest), You could use your own LLM (On prem)\n",
        "##Other techniques you can use to save cost is cache the responses for similar questions\n"
      ],
      "metadata": {
        "id": "gi6rzTt6_0Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing_extensions import ContextManager\n",
        "def test():\n",
        "    #query_text = 'Show me some developers who have also have business analysis'\n",
        "    query1 = 'what is Cholinesterase inhibitor?'\n",
        "    negResponse = \"I don't know\"\n",
        "\n",
        "    print (query1)\n",
        "    content = search1(query1)\n",
        "    print('resp: ', content)\n",
        "\n",
        "    prompt = f\"Answer this question in summarized bullets: {query1}\\n Provide information based on {content}\\nIf the answer is not contained in the supplied doc reply '{negResponse}' and nothing else\"\n",
        "        #print (prompt)\n",
        "\n",
        "    answer, word_count, openai_token_count = chat_gpt(prompt)\n",
        "        #print(f\"Response {i + 1}:\")\n",
        "    print(f\"----------------------------\\n\\n\\n\")\n",
        "    print(f\"Response from LLM: {content}\\nAnswer: {answer}\")\n",
        "    #print(f\"Word Count: {word_count}\")\n",
        "    #print(f\"OpenAI Token Count: {openai_token_count}\")\n",
        "test()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9deK9Xflq6rh",
        "outputId": "baf49c25-1a73-41ea-e5c1-55228b462321"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what is Cholinesterase inhibitor?\n",
            "Results from elastic: ['Description', 'Prescription', 'Prescription', 'Prescription'] ['Cholinesterase inhibitors', 'Donepezil', 'Galantamine', 'Rivastigmine '] ['Miscellaneous central nervous system agents', 'Memantine', 'Memantine', 'Memantine'] ['Dementia combinations', 'Donepezil-memantine', 'Donepezil-memantine', 'Donepezil-memantine']\n",
            "resp:  ['Description', 'Prescription', 'Prescription', 'Prescription'] ['Cholinesterase inhibitors', 'Donepezil', 'Galantamine', 'Rivastigmine '] ['Miscellaneous central nervous system agents', 'Memantine', 'Memantine', 'Memantine'] ['Dementia combinations', 'Donepezil-memantine', 'Donepezil-memantine', 'Donepezil-memantine']\n",
            "----------------------------\n",
            "\n",
            "\n",
            "\n",
            "Response from LLM: ['Description', 'Prescription', 'Prescription', 'Prescription'] ['Cholinesterase inhibitors', 'Donepezil', 'Galantamine', 'Rivastigmine '] ['Miscellaneous central nervous system agents', 'Memantine', 'Memantine', 'Memantine'] ['Dementia combinations', 'Donepezil-memantine', 'Donepezil-memantine', 'Donepezil-memantine']\n",
            "Answer: - Description: Cholinesterase inhibitors are medications used to treat dementia by increasing the levels of certain neurotransmitters in the brain.\n",
            "- Prescription: Donepezil, Galantamine, and Rivastigmine are common cholinesterase inhibitors prescribed for dementia.\n",
            "- Miscellaneous central nervous system agents: Memantine is another type of medication used for dementia, but it works differently from cholinesterase inhibitors.\n",
            "- Dementia combinations: Donepezil-memantine is a combination medication used for dementia that combines the effects of a cholinesterase inhibitor with memantine.\n"
          ]
        }
      ]
    }
  ]
}